{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ej-kIjP5d_zw",
    "outputId": "f8dd2ad4-a99c-4171-d92d-d3e5dc1360c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm==3.3.2\n",
      "  Downloading lightgbm-3.3.2-py3-none-manylinux1_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from lightgbm==3.3.2) (0.43.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lightgbm==3.3.2) (1.25.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lightgbm==3.3.2) (1.11.4)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in /usr/local/lib/python3.10/dist-packages (from lightgbm==3.3.2) (1.2.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn!=0.22.0->lightgbm==3.3.2) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn!=0.22.0->lightgbm==3.3.2) (3.5.0)\n",
      "Installing collected packages: lightgbm\n",
      "  Attempting uninstall: lightgbm\n",
      "    Found existing installation: lightgbm 4.1.0\n",
      "    Uninstalling lightgbm-4.1.0:\n",
      "      Successfully uninstalled lightgbm-4.1.0\n",
      "Successfully installed lightgbm-3.3.2\n"
     ]
    }
   ],
   "source": [
    "%pip install -U lightgbm==3.3.2\n",
    "# Light Gradient Boosting Machine, is a powerful and efficient gradient boosting framework developed by Microsoft.\n",
    "# it's designed for distributed and efficient training of large-scale machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yVREKx6JJqx1",
    "outputId": "068d3a72-0537-4dad-f742-7f245ca4c373"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting implicit\n",
      "  Downloading implicit-0.7.2-cp310-cp310-manylinux2014_x86_64.whl (8.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from implicit) (1.25.2)\n",
      "Requirement already satisfied: scipy>=0.16 in /usr/local/lib/python3.10/dist-packages (from implicit) (1.11.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from implicit) (4.66.4)\n",
      "Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.10/dist-packages (from implicit) (3.5.0)\n",
      "Installing collected packages: implicit\n",
      "Successfully installed implicit-0.7.2\n"
     ]
    }
   ],
   "source": [
    "%pip install implicit\n",
    "# implicit is a Python library for building and analyzing implicit feedback recommendation systems.\n",
    "# it provides implementations of collaborative filtering algorithms, including Alternating Least Squares (ALS), Bayesian Personalized Ranking (BPR) and Logistic Matrix Factorization (LMF)\n",
    "# which are commonly used in recommendation systems to generate recommendations based on implicit feedback (such as user interactions with items)\n",
    "# by installing the Implicit library, you gain access to these algorithms and can use them to build recommendation systems tailored to your specific use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Iq3oG1iwJan4",
    "outputId": "d92ebf1a-d0c0-4b52-c778-96d10720d63d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gboISq82XUN1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F6WO2VtEXefj"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import sys\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "# import modules in Python and configure the IPython interactive shell\n",
    "# IPython is an enhanced interactive Python interpreter, and it provides features like interactive computing, rich media integration, and support for data visualization\n",
    "# InteractiveShell is the core class responsible for handling interactive input and output, executing code, and managing user interactions in IPython environments\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sys.path.append(\"/content/drive/MyDrive/HM-new/\")  # path to the 'src' folder\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zxhDyWVbJR3w"
   },
   "outputs": [],
   "source": [
    "from src.data import DataHelper\n",
    "# imports the DataHelper class from the data module within the src package\n",
    "# it suggests that there is a module named data within the src package containing a class named DataHelper\n",
    "\n",
    "from src.data.metrics import map_at_k, hr_at_k, recall_at_k\n",
    "# imports the map_at_k, hr_at_k, and recall_at_k functions from the metrics module within the data package of the src package\n",
    "\n",
    "from src.retrieval.rules import (\n",
    "    OrderHistory,\n",
    "    OrderHistoryDecay,\n",
    "    ItemPair,\n",
    "    UserGroupTimeHistory,\n",
    "    UserGroupSaleTrend,\n",
    "    TimeHistory,\n",
    "    TimeHistoryDecay,\n",
    "    SaleTrend,\n",
    "    OutOfStock,\n",
    ")\n",
    "# imports multiple classes from the rules module within the retrieval package of the src package\n",
    "\n",
    "from src.retrieval.collector import RuleCollector\n",
    "# imports the RuleCollector class from the collector module within the retrieval package of the src package\n",
    "\n",
    "from src.features import full_sale, week_sale, repurchase_ratio, popularity, period_sale\n",
    "# imports multiple functions from the features module within the src package\n",
    "# it imports full_sale, week_sale, repurchase_ratio, popularity, and period_sale functions\n",
    "\n",
    "from src.utils import (\n",
    "    calc_valid_date,\n",
    "    merge_week_data,\n",
    "    reduce_mem_usage,\n",
    "    calc_embd_similarity,\n",
    ")\n",
    "# imports multiple functions from the utils module within the src package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lL6IRSJ2JR3x"
   },
   "outputs": [],
   "source": [
    "data_dir = Path(\"/content/drive/MyDrive/HM-new/data/\")\n",
    "model_dir = Path(\"/content/drive/MyDrive/HM-new/models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AakTfKUQJR3x"
   },
   "outputs": [],
   "source": [
    "TRAIN_WEEK_NUM = 4\n",
    "WEEK_NUM = TRAIN_WEEK_NUM + 2\n",
    "\n",
    "VERSION_NAME = \"Recall 1\"\n",
    "TEST = True  # Set as `False` when do local experiments to save time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RHu8rsZMJR3y"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(data_dir/\"interim\"/VERSION_NAME):\n",
    "    os.mkdir(data_dir/\"interim\"/VERSION_NAME)\n",
    "if not os.path.exists(data_dir/\"processed\"/VERSION_NAME):\n",
    "    os.mkdir(data_dir/\"processed\"/VERSION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2BPv2lcWGGQJ"
   },
   "outputs": [],
   "source": [
    "dh = DataHelper(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kiIB1uvjJR33",
    "outputId": "69bb0f82-971c-469d-dd52-5fa8116a347d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encode User Sparse Feats: 100%|██████████| 5/5 [00:04<00:00,  1.08it/s]\n",
      "Encode Item Sparse Feats: 100%|██████████| 12/12 [00:00<00:00, 13.33it/s]\n"
     ]
    }
   ],
   "source": [
    "data = dh.preprocess_data(save=True, name=\"encoded_full\")  # run only once, processed data will be saved\n",
    "# preprocesses the data using the preprocess_data method of the DataHelper class, saves the processed data to disk and assigns the processed data to the variable data\n",
    "# it's mentioned that this operation should be run only once, as the processed data will be saved and can be reused in subsequent executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i3ngtb7NUGbH"
   },
   "outputs": [],
   "source": [
    "data = dh.load_data(name=\"encoded_full\")\n",
    "# loads the data named \"encoded_full\" using the load_data method of the DataHelper class and assigns it to the variable data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ecaPObIJR34"
   },
   "outputs": [],
   "source": [
    "uid2idx = pickle.load(open(data_dir/\"index_id_map/user_id2index.pkl\", \"rb\"))\n",
    "submission = pd.read_csv(data_dir/\"raw\"/'sample_submission.csv')\n",
    "submission['customer_id'] = submission['customer_id'].map(uid2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LX4M190s4pxo"
   },
   "source": [
    "## Retrieval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcShMOQPJR35"
   },
   "source": [
    "Generate candidates for each week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J7gu-mwjJR36"
   },
   "outputs": [],
   "source": [
    "listBin = [-1, 19, 29, 39, 49, 59, 69, 119]\n",
    "data['user']['age_bins'] = pd.cut(data['user']['age'], listBin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "glOiftg3JR36",
    "outputId": "1bc53373-b449-40c5-c53b-4527808c9807"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Week 1: [2020-09-16, 2020-09-23)\n"
     ]
    }
   ],
   "source": [
    "# WEEK_NUM = 0: test\n",
    "# WEEK_NUM = 1: valid\n",
    "# WEEK_NUM > 1: train\n",
    "for week in range(1,WEEK_NUM):\n",
    "    # use sliding window to generate candidates\n",
    "    if week == 0 and not TEST:\n",
    "        continue\n",
    "    trans = data[\"inter\"]\n",
    "\n",
    "    start_date, end_date = calc_valid_date(week)\n",
    "    print(f\"Week {week}: [{start_date}, {end_date})\")\n",
    "\n",
    "    train, valid = dh.split_data(trans, start_date, end_date)\n",
    "    train = train.merge(data['user'][['customer_id','age_bins']], on='customer_id', how='left')\n",
    "\n",
    "    last_week_start = pd.to_datetime(start_date) - pd.Timedelta(days=7)\n",
    "    last_week_start = last_week_start.strftime(\"%Y-%m-%d\")\n",
    "    last_week = train.loc[train.t_dat >= last_week_start]\n",
    "\n",
    "    last_3day_start = pd.to_datetime(start_date) - pd.Timedelta(days=3)\n",
    "    last_3day_start = last_3day_start.strftime(\"%Y-%m-%d\")\n",
    "    last_3days = train.loc[train.t_dat >= last_3day_start]\n",
    "\n",
    "    if week != 0:\n",
    "        customer_list = valid[\"customer_id\"].values\n",
    "    else:\n",
    "        customer_list = submission['customer_id'].values\n",
    "\n",
    "    # ========================== Retrieval Strategies ==========================\n",
    "\n",
    "    candidates = RuleCollector().collect(\n",
    "        week_num = week,\n",
    "        trans_df = trans,\n",
    "        customer_list=customer_list,\n",
    "        rules=[\n",
    "            OrderHistory(train, days=3, name='1'),\n",
    "            OrderHistory(train, days=7, name='2'),\n",
    "            OrderHistoryDecay(train, days=3, n=50, name='1'),\n",
    "            OrderHistoryDecay(train, days=7, n=50, name='2'),\n",
    "            ItemPair(OrderHistory(train, days=3).retrieve(), name='1'),\n",
    "            ItemPair(OrderHistory(train, days=7).retrieve(), name='2'),\n",
    "            ItemPair(OrderHistoryDecay(train, days=3, n=50).retrieve(), name='3'),\n",
    "            ItemPair(OrderHistoryDecay(train, days=7, n=50).retrieve(), name='4'),\n",
    "            UserGroupTimeHistory(data, customer_list, last_week, ['age_bins'], n=50, name='1'),\n",
    "            UserGroupTimeHistory(data, customer_list, last_3days, ['age_bins'], n=50, name='2'),\n",
    "            UserGroupSaleTrend(data, customer_list, train, ['age_bins'], days=7, n=50),\n",
    "            TimeHistory(customer_list, last_week, n=50, name='1'),\n",
    "            TimeHistory(customer_list, last_3days, n=50, name='2'),\n",
    "            TimeHistoryDecay(customer_list, train, days=3, n=50, name='1'),\n",
    "            TimeHistoryDecay(customer_list, train, days=7, n=50, name='2'),\n",
    "            SaleTrend(customer_list, train, days=7, n=50),\n",
    "        ],\n",
    "        filters=[OutOfStock(trans)],\n",
    "        min_pos_rate=0.006,\n",
    "        compress=False,\n",
    "    )\n",
    "\n",
    "    candidates = (\n",
    "        pd.pivot_table(\n",
    "            candidates,\n",
    "            values=\"score\",\n",
    "            index=[\"customer_id\", \"article_id\"],\n",
    "            columns=[\"method\"],\n",
    "            aggfunc=np.sum,\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    candidates.to_parquet(data_dir/\"interim\"/VERSION_NAME/f\"week{week}_candidate.pqt\")\n",
    "    valid.to_parquet(data_dir/\"processed\"/VERSION_NAME/f\"week{week}_label.pqt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3wqDho1ZJR38"
   },
   "outputs": [],
   "source": [
    "# use the threshold in week 1 to generate candidates for test data, see the log in the upper cell\n",
    "if TEST:\n",
    "    week = 0\n",
    "    trans = data[\"inter\"]\n",
    "\n",
    "    start_date, end_date = calc_valid_date(week)\n",
    "    print(f\"Week {week}: [{start_date}, {end_date})\")\n",
    "\n",
    "    train, valid = dh.split_data(trans, start_date, end_date)\n",
    "    train = train.merge(data['user'][['customer_id','age_bins']], on='customer_id', how='left')\n",
    "\n",
    "    last_week_start = pd.to_datetime(start_date) - pd.Timedelta(days=7)\n",
    "    last_week_start = last_week_start.strftime(\"%Y-%m-%d\")\n",
    "    last_week = train.loc[train.t_dat >= last_week_start]\n",
    "\n",
    "    last_3day_start = pd.to_datetime(start_date) - pd.Timedelta(days=3)\n",
    "    last_3day_start = last_3day_start.strftime(\"%Y-%m-%d\")\n",
    "    last_3days = train.loc[train.t_dat >= last_3day_start]\n",
    "\n",
    "    customer_list = submission['customer_id'].values\n",
    "\n",
    "    # ========================== Retrieval Strategies ==========================\n",
    "\n",
    "    candidates = RuleCollector().collect(\n",
    "        week_num = week,\n",
    "        trans_df = trans,\n",
    "        customer_list=customer_list,\n",
    "        rules=[\n",
    "            OrderHistory(train, days=3, name='1'),\n",
    "            OrderHistory(train, days=7, name='2'),\n",
    "            OrderHistoryDecay(train, days=3, n=50, name='1'),\n",
    "            OrderHistoryDecay(train, days=7, n=50, name='2'),\n",
    "            ItemPair(OrderHistory(train, days=3).retrieve(), name='1'),\n",
    "            ItemPair(OrderHistory(train, days=7).retrieve(), name='2'),\n",
    "            ItemPair(OrderHistoryDecay(train, 3, n=50).retrieve(), name='3'),\n",
    "            ItemPair(OrderHistoryDecay(train, 7, n=50).retrieve(), name='4'),\n",
    "            UserGroupTimeHistory(data, customer_list, last_week, ['age_bins'], n=15, name='1'),\n",
    "            UserGroupTimeHistory(data, customer_list, last_3days, ['age_bins'], n=20.5, name='2'),\n",
    "            UserGroupSaleTrend(data, customer_list, train, ['age_bins'], days=7, n=2),\n",
    "            TimeHistory(customer_list, last_week, n=9, name='1'),\n",
    "            TimeHistory(customer_list, last_3days, n=16, name='2'),\n",
    "            TimeHistoryDecay(customer_list, train, days=3, n=12, name='1'),\n",
    "            TimeHistoryDecay(customer_list, train, days=7, n=8, name='2'),\n",
    "            SaleTrend(customer_list, train, days=7, n=2),\n",
    "        ],\n",
    "        filters=[OutOfStock(trans)],\n",
    "        min_pos_rate=0.006,\n",
    "        compress=False,\n",
    "    )\n",
    "\n",
    "    candidates, _ = reduce_mem_usage(candidates)\n",
    "    candidates = (\n",
    "        pd.pivot_table(\n",
    "            candidates,\n",
    "            values=\"score\",\n",
    "            index=[\"customer_id\", \"article_id\"],\n",
    "            columns=[\"method\"],\n",
    "            aggfunc=np.sum,\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    candidates.to_parquet(data_dir/\"interim\"/VERSION_NAME/f\"week{week}_candidate.pqt\")\n",
    "    valid.to_parquet(data_dir/\"processed\"/VERSION_NAME/f\"week{week}_label.pqt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gQEog9JJJR39"
   },
   "outputs": [],
   "source": [
    "del train, valid, last_week, customer_list, candidates\n",
    "gc.collect()\n",
    "# clear up memory usage by deleting variables and collecting garbage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TBIrKxhRGGQR"
   },
   "source": [
    "## Feature engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9pF-sOMSGGQP"
   },
   "outputs": [],
   "source": [
    "user = data[\"user\"]\n",
    "item = data[\"item\"]\n",
    "inter = data[\"inter\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "heKIAtoRdtIk"
   },
   "outputs": [],
   "source": [
    "# calculate week number\n",
    "inter['week'] = (pd.to_datetime('2020-09-29') - pd.to_datetime(inter['t_dat'])).dt.days // 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kaEwbD6KY5R8"
   },
   "outputs": [],
   "source": [
    "# merge full candidates to transaction data (avoid feature missing in training data)\n",
    "full_candidates = []\n",
    "for i in tqdm(range(WEEK_NUM)):\n",
    "    candidate = pd.read_parquet(data_dir/\"interim\"/VERSION_NAME/f\"week{i}_candidate.pqt\")\n",
    "    full_candidates += candidate['article_id'].values.tolist()\n",
    "full_candidates = list(set(full_candidates))\n",
    "del candidate\n",
    "gc.collect()\n",
    "\n",
    "num_candidates = len(full_candidates)\n",
    "full_candidates = np.array(full_candidates)\n",
    "full_candidates = np.tile(full_candidates, WEEK_NUM + 1)\n",
    "weeks = np.repeat(np.arange(1,WEEK_NUM+2), num_candidates)\n",
    "full_candidates = pd.DataFrame({'article_id':full_candidates, 'week':weeks})\n",
    "\n",
    "inter['valid'] = 1\n",
    "in_train = inter[inter['week']<=WEEK_NUM + 1]\n",
    "out_train = inter[inter['week']>WEEK_NUM + 1]\n",
    "\n",
    "in_train = in_train.merge(full_candidates, on=['article_id','week'], how='right')\n",
    "in_train['valid'] = in_train['valid'].fillna(0)\n",
    "inter = pd.concat([in_train, out_train], ignore_index=True)\n",
    "inter = inter.sort_values([\"valid\"], ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QsW8gPPUUUg_"
   },
   "outputs": [],
   "source": [
    "# merge `product_code`\n",
    "inter = inter.merge(item[[\"article_id\", \"product_code\"]], on=\"article_id\", how=\"left\")\n",
    "# merges the product_code information from the item DataFrame into the inter DataFrame based on the common column article_id\n",
    "# after this operation, the inter DataFrame will have an additional column product_code, containing the corresponding product codes for each article ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hz4m4zsiieqn"
   },
   "outputs": [],
   "source": [
    "inter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kYdOqlyldvNM"
   },
   "outputs": [],
   "source": [
    "_, inter[\"i_1w_sale_rank\"], inter[\"i_1w_sale_norm\"] = period_sale(\n",
    "    inter, [\"article_id\"], days=14, rank=True, norm=True, week_num=WEEK_NUM\n",
    ")\n",
    "_, inter[\"p_1w_sale_rank\"], inter[\"p_1w_sale_norm\"] = period_sale(\n",
    "    inter, [\"product_code\"], days=14, rank=True, norm=True, week_num=WEEK_NUM\n",
    ")\n",
    "inter[\"i_2w_sale\"], inter[\"i_2w_sale_rank\"], inter[\"i_2w_sale_norm\"] = period_sale(\n",
    "    inter, [\"article_id\"], days=14, rank=True, norm=True, week_num=WEEK_NUM\n",
    ")\n",
    "inter[\"p_2w_sale\"], inter[\"p_2w_sale_rank\"], inter[\"p_2w_sale_norm\"] = period_sale(\n",
    "    inter, [\"product_code\"], days=14, rank=True, norm=True, week_num=WEEK_NUM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Lv1FtoOeI9n"
   },
   "outputs": [],
   "source": [
    "inter[\"i_3w_sale\"], inter[\"i_3w_sale_rank\"], inter[\"i_3w_sale_norm\"] = period_sale(\n",
    "    inter, [\"article_id\"], days=21, rank=True, norm=True, week_num=WEEK_NUM\n",
    ")\n",
    "inter[\"p_3w_sale\"], inter[\"p_3w_sale_rank\"], inter[\"p_3w_sale_norm\"] = period_sale(\n",
    "    inter, [\"product_code\"], days=21, rank=True, norm=True, week_num=WEEK_NUM\n",
    ")\n",
    "inter[\"i_4w_sale\"], inter[\"i_4w_sale_rank\"], inter[\"i_4w_sale_norm\"] = period_sale(\n",
    "    inter, [\"article_id\"], days=28, rank=True, norm=True, week_num=WEEK_NUM\n",
    ")\n",
    "inter[\"p_4w_sale\"], inter[\"p_4w_sale_rank\"], inter[\"p_4w_sale_norm\"] = period_sale(\n",
    "    inter, [\"product_code\"], days=28, rank=True, norm=True, week_num=WEEK_NUM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Yqb3KaWinZM"
   },
   "outputs": [],
   "source": [
    "inter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FdlZqq5QzbMR"
   },
   "outputs": [],
   "source": [
    "inter['i_repurchase_ratio'] = repurchase_ratio(inter, ['article_id'], week_num=WEEK_NUM)\n",
    "inter['p_repurchase_ratio'] = repurchase_ratio(inter, ['product_code'], week_num=WEEK_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oh3Otv_Cipmi"
   },
   "outputs": [],
   "source": [
    "inter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "riX-F5H5lbMu"
   },
   "outputs": [],
   "source": [
    "inter, _ = reduce_mem_usage(inter)\n",
    "# reduces the memory usage of the DataFrame inter using the reduce_mem_usage function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "70okKvWfzgip"
   },
   "outputs": [],
   "source": [
    "inter[\"i_sale\"] = week_sale(inter, [\"article_id\"], week_num=WEEK_NUM)\n",
    "inter[\"p_sale\"] = week_sale(inter, [\"product_code\"], week_num=WEEK_NUM)\n",
    "inter[\"i_sale_uni\"] = week_sale(inter, [\"article_id\"], True, week_num=WEEK_NUM)\n",
    "inter[\"p_sale_uni\"] = week_sale(inter, [\"product_code\"], True, week_num=WEEK_NUM)\n",
    "inter[\"lw_i_sale\"] = week_sale(inter, [\"article_id\"], step=1, week_num=WEEK_NUM) # * last week sale\n",
    "inter[\"lw_p_sale\"] = week_sale(inter, [\"product_code\"], step=1, week_num=WEEK_NUM)\n",
    "inter[\"lw_i_sale_uni\"] = week_sale(inter, [\"article_id\"], True, step=1, week_num=WEEK_NUM)\n",
    "inter[\"lw_p_sale_uni\"] = week_sale(inter, [\"product_code\"], True, step=1, week_num=WEEK_NUM)\n",
    "\n",
    "inter[\"i_sale_ratio\"] = inter[\"i_sale\"] / (inter[\"p_sale\"] + 1e-6)\n",
    "inter[\"i_sale_uni_ratio\"] = inter[\"i_sale_uni\"] / (inter[\"p_sale_uni\"] + 1e-6)\n",
    "inter[\"lw_i_sale_ratio\"] = inter[\"lw_i_sale\"] / (inter[\"lw_p_sale\"] + 1e-6)\n",
    "inter[\"lw_i_sale_uni_ratio\"] = inter[\"lw_i_sale_uni\"] / (inter[\"lw_p_sale_uni\"] + 1e-6)\n",
    "\n",
    "inter[\"i_uni_ratio\"] = inter[\"i_sale\"] / (inter[\"i_sale_uni\"] + 1e-6)\n",
    "inter[\"p_uni_ratio\"] = inter[\"p_sale\"] / (inter[\"p_sale_uni\"] + 1e-6)\n",
    "inter[\"lw_i_uni_ratio\"] = inter[\"lw_i_sale\"] / (inter[\"lw_i_sale_uni\"] + 1e-6)\n",
    "inter[\"lw_p_uni_ratio\"] = inter[\"lw_p_sale\"] / (inter[\"lw_p_sale_uni\"] + 1e-6)\n",
    "\n",
    "inter[\"i_sale_trend\"] = (inter[\"i_sale\"] - inter[\"lw_i_sale\"]) / (inter[\"lw_i_sale\"] + 1e-6)\n",
    "inter[\"p_sale_trend\"] = (inter[\"p_sale\"] - inter[\"lw_p_sale\"]) / (inter[\"lw_p_sale\"] + 1e-6)\n",
    "\n",
    "item_feats = [\n",
    "    \"product_type_no\",\n",
    "    # \"product_group_name\",\n",
    "    # \"graphical_appearance_no\",\n",
    "    # \"colour_group_code\",\n",
    "    # \"perceived_colour_value_id\",\n",
    "    # \"perceived_colour_master_id\",\n",
    "]\n",
    "inter = inter.merge(item[[\"article_id\", *item_feats]], on=\"article_id\", how=\"left\")\n",
    "\n",
    "for f in tqdm(item_feats):\n",
    "    inter[f\"{f}_sale\"] = week_sale(inter, [f], f\"{f}_sale\", week_num=WEEK_NUM)\n",
    "    inter[f\"lw_{f}_sale\"] = week_sale(inter, [f], f\"{f}_sale\", step=1, week_num=WEEK_NUM)\n",
    "    inter[f\"{f}_sale_trend\"] = (inter[f\"{f}_sale\"] - inter[f\"lw_{f}_sale\"]) / (inter[f\"lw_{f}_sale\"] + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K4hO0oT3isNu"
   },
   "outputs": [],
   "source": [
    "inter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mQkGNyjaBbi-"
   },
   "outputs": [],
   "source": [
    "# Date related\n",
    "curr_date_dict = {x:calc_valid_date(x-1)[0] for x in range(100)}\n",
    "current_dat = inter['week'].map(curr_date_dict)\n",
    "mask = inter['valid']==0\n",
    "inter.loc[mask, 't_dat'] = inter.loc[mask, 'week'].map(curr_date_dict)\n",
    "first_date = inter.groupby('article_id')['t_dat'].min().reset_index(name='first_dat')\n",
    "inter = pd.merge(inter, first_date, on='article_id', how='left')\n",
    "# df = pd.merge(df, last_date, on='article_id', how='left')\n",
    "inter['first_dat'] = (pd.to_datetime(current_dat)-pd.to_datetime(inter['first_dat'])).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sZHdSXtAi_zK"
   },
   "outputs": [],
   "source": [
    "inter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OC8SIYK_Ca-g"
   },
   "outputs": [],
   "source": [
    "inter['i_full_sale'] = full_sale(inter, ['article_id'], week_num=WEEK_NUM)\n",
    "inter['p_full_sale'] = full_sale(inter, ['product_code'], week_num=WEEK_NUM)\n",
    "\n",
    "inter['i_daily_sale'] = inter['i_full_sale'] / inter['first_dat']\n",
    "inter['p_daily_sale'] = inter['p_full_sale'] / inter['first_dat']\n",
    "inter['i_daily_sale_ratio'] = inter['i_daily_sale'] / inter['p_daily_sale']\n",
    "inter['i_w_full_sale_ratio'] = inter['i_sale'] / inter['i_full_sale']\n",
    "\n",
    "inter['i_2w_full_sale_ratio'] = inter['i_2w_sale'] / inter['i_full_sale']\n",
    "inter['p_w_full_sale_ratio'] = inter['p_sale'] / inter['p_full_sale']\n",
    "inter['p_2w_full_sale_ratio'] = inter['p_2w_sale'] / inter['p_full_sale']\n",
    "\n",
    "inter['i_week_above_daily_sale'] = inter['i_sale'] / 7 - inter['i_daily_sale']\n",
    "inter['p_week_above_full_sale'] = inter['p_sale'] / 7 - inter['i_full_sale']\n",
    "inter['i_2w_week_above_daily_sale'] = inter['i_2w_sale'] / 14 - inter['i_daily_sale']\n",
    "inter['p_2w_week_above_daily_sale'] = inter['p_2w_sale'] / 14 - inter['p_daily_sale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "334plAMZ5Ics"
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z4HdQYbZ5CC-"
   },
   "outputs": [],
   "source": [
    "for f in tqdm(item_feats):\n",
    "    inter[f'{f}_full_sale'] = full_sale(inter, [f], week_num=WEEK_NUM)\n",
    "    f_first_date = inter.groupby(f)['t_dat'].min().reset_index(name=f'{f}_first_dat')\n",
    "    inter = inter.merge(f_first_date, on=f, how='left')\n",
    "    inter[f'{f}_daily_sale'] = inter[f'{f}_full_sale'] / (pd.to_datetime(current_dat) - pd.to_datetime(inter[f'{f}_first_dat'])).dt.days\n",
    "    inter[f'i_{f}_daily_sale_ratio'] = inter['i_daily_sale'] / inter[f'{f}_daily_sale']\n",
    "    inter[f'p_{f}_daily_sale_ratio'] = inter['p_daily_sale'] / inter[f'{f}_daily_sale']\n",
    "    del inter[f'{f}_full_sale'], inter[f'{f}_first_dat']\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_fhlw8z4z5hD"
   },
   "outputs": [],
   "source": [
    "for f in item_feats + ['i_full_sale','p_full_sale']:\n",
    "    del inter[f]\n",
    "# this code block deletes the columns specified in the `item_feats` list along with `i_full_sale` and `p_full_sale` from the `inter` DataFrame.\n",
    "# this step is likely taken to clean up unnecessary columns and optimize memory usage after the feature engineering process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ms4s0YUJznFU"
   },
   "outputs": [],
   "source": [
    "inter['i_pop'] = popularity(inter, 'article_id', week_num=WEEK_NUM)\n",
    "inter['p_pop'] = popularity(inter, 'product_code', week_num=WEEK_NUM)\n",
    "# the code adds two new columns to the `inter` DataFrame, namely `i_pop` and `p_pop`, which represent the popularity of items and product codes, respectively\n",
    "# the popularity is calculated based on the number of occurrences of each item or product code within the specified time window, which is determined by the `week_num` parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ip6AdvV0kS1y"
   },
   "outputs": [],
   "source": [
    "inter = inter.loc[inter['week'] <= WEEK_NUM + 2]\n",
    "# filters the inter DataFrame to include only rows where the week number is less than or equal to WEEK_NUM + 2\n",
    "# adding two more weeks to WEEK_NUM extends the analysis or processing for an additional two weeks beyond the original duration specified by WEEK_NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "huYRSLE91PJH"
   },
   "outputs": [],
   "source": [
    "inter.to_parquet(data_dir / \"processed/processed_inter.pqt\")\n",
    "# saves the DataFrame inter to a Parquet file named \"processed_inter.pqt\" in the specified data directory\n",
    "# parquet is a columnar storage file format, often used in big data processing frameworks like Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jywhBCAGGGQS"
   },
   "source": [
    "## Merge Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YwbQ4jyD4MxX"
   },
   "outputs": [],
   "source": [
    "inter = pd.read_parquet(data_dir / \"processed/processed_inter.pqt\")\n",
    "inter = inter[inter['week'] <= WEEK_NUM + 2]\n",
    "# reads the processed DataFrame inter from the Parquet file \"processed_inter.pqt\" located in the specified data directory\n",
    "# it then filters the DataFrame to include only rows where the week number (week) is less than or equal to the value of WEEK_NUM + 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-7YlHho_JR4F"
   },
   "outputs": [],
   "source": [
    "# embeddings from DSSM model\n",
    "dssm_user_embd = np.load(data_dir / \"external/dssm_user_embd.npy\", allow_pickle=True)\n",
    "dssm_item_embd = np.load(data_dir / \"external/dssm_item_embd.npy\", allow_pickle=True)\n",
    "# embeddings from YouTubeDNN model\n",
    "yt_user_embd = np.load(data_dir / \"external/yt_user_embd.npy\", allow_pickle=True)\n",
    "yt_item_embd = np.load(data_dir / \"external/yt_item_embd.npy\", allow_pickle=True)\n",
    "# embeddings from Word2Vector model\n",
    "w2v_user_embd = np.load(data_dir/'external'/'w2v_user_embd.npy', allow_pickle=True)\n",
    "w2v_item_embd = np.load(data_dir/'external'/'w2v_item_embd.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IveB8V0VJGLm"
   },
   "outputs": [],
   "source": [
    "for col in inter.columns:\n",
    "    inter[col] = np.nan_to_num(inter[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HXmOMDkO33sr"
   },
   "outputs": [],
   "source": [
    "for i in tqdm(range(WEEK_NUM)):\n",
    "    if i == 0 and not TEST:\n",
    "        continue\n",
    "    candidate = pd.read_parquet(data_dir/\"interim\"/VERSION_NAME/f\"week{i}_candidate.pqt\")\n",
    "    if i == 0:\n",
    "        chunk_size = int(candidate.shape[0] * 0.5)\n",
    "        for chunk,batch in enumerate(range(0, candidate.shape[0], chunk_size)):\n",
    "            sub_candidate = candidate.iloc[batch:batch+chunk_size-1]\n",
    "            # * merge features\n",
    "            sub_candidate = merge_week_data(data, inter, i, sub_candidate)\n",
    "            sub_candidate['article_id'] = sub_candidate['article_id'].astype(int)\n",
    "            sub_candidate['customer_id'] = sub_candidate['customer_id'].astype(int)\n",
    "            # * merge DSSM user and item embeddings\n",
    "            sub_candidate[\"dssm_similarity\"] = calc_embd_similarity(sub_candidate, dssm_user_embd, dssm_item_embd)\n",
    "            # * merge YouTubeDNN user and item embeddings\n",
    "            sub_candidate[\"yt_similarity\"] = calc_embd_similarity(sub_candidate, yt_user_embd, yt_item_embd)\n",
    "            # * merge Word2Vector user and item embeddings\n",
    "            sub_candidate[\"wv_similarity\"] = calc_embd_similarity(sub_candidate, w2v_user_embd, w2v_item_embd, sub=False)\n",
    "            print(f\"Chunk {chunk} done...\")\n",
    "            sub_candidate.to_parquet(data_dir/\"processed\"/VERSION_NAME/f\"week{i}_candidate_{chunk}.pqt\")\n",
    "    else:\n",
    "        # * merge features\n",
    "        candidate = merge_week_data(data, inter, i, candidate)\n",
    "        print(candidate['week'].unique())\n",
    "        # * merge DSSM user and item embeddings\n",
    "        candidate[\"dssm_similarity\"] = calc_embd_similarity(candidate, dssm_user_embd, dssm_item_embd)\n",
    "        # * merge YouTubeDNN user and item embeddings\n",
    "        candidate[\"yt_similarity\"] = calc_embd_similarity(candidate, yt_user_embd, yt_item_embd)\n",
    "        candidate[\"wv_similarity\"] = calc_embd_similarity(candidate, w2v_user_embd, w2v_item_embd, sub=False)\n",
    "    candidate.to_parquet(data_dir/\"processed\"/VERSION_NAME/f\"week{i}_candidate.pqt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sWyKQBAt3x--"
   },
   "outputs": [],
   "source": [
    "del dssm_user_embd, dssm_item_embd, yt_user_embd, yt_item_embd\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Zml-cxryoU_"
   },
   "source": [
    "## Ranking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PsmE6rgjdtIr"
   },
   "outputs": [],
   "source": [
    "candidates = {}\n",
    "labels = {}\n",
    "for i in tqdm(range(1, WEEK_NUM)):\n",
    "    candidates[i] = pd.read_parquet(data_dir/\"processed\"/VERSION_NAME/f\"week{i}_candidate.pqt\")\n",
    "    labels[i] = pd.read_parquet(data_dir/\"processed\"/VERSION_NAME/f\"week{i}_label.pqt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xWqYwCbsdtIr"
   },
   "outputs": [],
   "source": [
    "feats = [\n",
    "    x\n",
    "    for x in candidates[1].columns\n",
    "    if x\n",
    "    not in [\n",
    "        \"label\",\n",
    "        \"sales_channel_id\",\n",
    "        \"t_dat\",\n",
    "        \"week\",\n",
    "    ]\n",
    "]\n",
    "cat_features = [\n",
    "    \"customer_id\",\n",
    "    \"article_id\",\n",
    "    \"product_code\",\n",
    "    \"FN\",\n",
    "    \"Active\",\n",
    "    \"club_member_status\",\n",
    "    \"fashion_news_frequency\",\n",
    "    \"age\",\n",
    "    \"product_type_no\",\n",
    "    \"product_group_name\",\n",
    "    \"graphical_appearance_no\",\n",
    "    \"colour_group_code\",\n",
    "    \"perceived_colour_value_id\",\n",
    "    \"perceived_colour_master_id\",\n",
    "\n",
    "    \"user_gender\",\n",
    "    \"article_gender\",\n",
    "    \"season_type\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cweXHQMrJR4I"
   },
   "outputs": [],
   "source": [
    "# Convert categorical featues as `CategoricalDtype`\n",
    "cate_dict = {}\n",
    "for feat in tqdm(cat_features):\n",
    "    if feat in data['user'].columns:\n",
    "        value_set = set(data['user'][feat].unique())\n",
    "    elif feat in data['item'].columns:\n",
    "        value_set = set(data['item'][feat].unique())\n",
    "    else:\n",
    "        value_set = set(data['inter'][feat].unique())\n",
    "    cate_dict[feat] = CategoricalDtype(categories=value_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G-fVjg3JT_OE"
   },
   "outputs": [],
   "source": [
    "full_data = pd.concat([candidates[i] for i in range(1, WEEK_NUM)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLttJ3CtJ39a"
   },
   "source": [
    "### Extra Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lcsSONupJ9zO"
   },
   "outputs": [],
   "source": [
    "inter = data['inter']\n",
    "inter = inter[inter['t_dat']<'2020-08-19']   # start date of the last valid week\n",
    "inter['week'] = (pd.to_datetime('2020-09-29') - pd.to_datetime(inter['t_dat'])).dt.days // 7\n",
    "inter = inter.merge(data['item'][[\"article_id\", \"product_code\"]], on=\"article_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z3vaGE2nJ-zT"
   },
   "outputs": [],
   "source": [
    "tmp = inter.groupby('article_id').week.mean()\n",
    "full_data['article_time_mean'] = full_data['article_id'].map(tmp)\n",
    "\n",
    "tmp = inter.groupby('customer_id').week.nth(-1)\n",
    "full_data['customer_id_last_time'] = full_data['customer_id'].map(tmp)\n",
    "\n",
    "tmp = inter.groupby('customer_id').week.nth(0)\n",
    "full_data['customer_id_first_time'] = full_data['customer_id'].map(tmp)\n",
    "\n",
    "tmp = inter.groupby('customer_id').week.mean()\n",
    "full_data['customer_id_time_mean'] = full_data['customer_id'].map(tmp)\n",
    "\n",
    "full_data['customer_id_gap'] = full_data['customer_id_first_time'] - full_data['customer_id_last_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RYEfzdHmczFn"
   },
   "outputs": [],
   "source": [
    "feats += [\n",
    "    'article_time_mean',\n",
    "    'customer_id_last_time',\n",
    "    'customer_id_first_time',\n",
    "    'customer_id_time_mean',\n",
    "    'customer_id_gap'\n",
    "]\n",
    "\n",
    "# this code adds the newly created temporal features to the list of features `feats`. These features will be used in subsequent modeling steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "psZK0985KCnl"
   },
   "outputs": [],
   "source": [
    "del tmp\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3RojtbK9GGQU"
   },
   "source": [
    "### Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4UeLfwqRc4iQ"
   },
   "outputs": [],
   "source": [
    "for feat in tqdm(cat_features):\n",
    "    full_data[feat] = full_data[feat].astype(cate_dict[feat])\n",
    "\n",
    "# this code converts the categorical features in `full_data` to the specified categorical data type (`CategoricalDtype`) using the dictionary `cate_dict` created earlier\n",
    "# this is done to optimize memory usage and speed up certain operations when working with categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rJIW8LACUCmg"
   },
   "outputs": [],
   "source": [
    "train = full_data.loc[full_data['week']>1]\n",
    "valid = full_data.loc[full_data['week']==1]\n",
    "\n",
    "del full_data\n",
    "gc.collect()\n",
    "\n",
    "# separates the `full_data` into `train` and `valid` sets based on the week column.\n",
    "# specifically, it selects rows where the week is greater than 1 for the training set (`train`) and where the week is equal to 1 for the validation set (`valid`).\n",
    "# finally, it deletes the `full_data` dataframe from memory and performs garbage collection to free up memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nTm9bz6vR6T4"
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"objective\": \"binary\",       # Objective for binary classification\n",
    "    \"boosting_type\": \"gbdt\",     # Gradient Boosting Decision Tree\n",
    "    \"metric\": \"auc\",             # Evaluation metric: Area Under the Curve\n",
    "    \"max_depth\": 8,              # Maximum tree depth for base learners\n",
    "    \"num_leaves\": 125,           # Maximum number of leaves in one tree\n",
    "    \"learning_rate\": 0.03,       # Learning rate\n",
    "    \"verbose\": -1,               # Suppress output\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G2wtl4vo9F8J"
   },
   "outputs": [],
   "source": [
    "def train_classifier_model(train, valid):\n",
    "\n",
    "    train_set = lgb.Dataset(\n",
    "        data=train[feats],\n",
    "        label=train[\"label\"],\n",
    "        feature_name=feats,\n",
    "        categorical_feature=cat_features,\n",
    "        params=params,\n",
    "    )\n",
    "\n",
    "    valid_set = lgb.Dataset(\n",
    "        data=valid[feats],\n",
    "        label=valid[\"label\"],\n",
    "        feature_name=feats,\n",
    "        categorical_feature=cat_features,\n",
    "        params=params,\n",
    "    )\n",
    "\n",
    "    classifier = lgb.train(\n",
    "        params,\n",
    "        train_set,\n",
    "        num_boost_round=300,\n",
    "        valid_sets=[valid_set],\n",
    "        early_stopping_rounds=30,\n",
    "        verbose_eval=10,\n",
    "    )\n",
    "    classifier.save_model(\n",
    "        model_dir / f\"lgb_classifier.model\",\n",
    "        num_iteration=classifier.best_iteration,\n",
    "    )\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vw2PmIZwPdFf"
   },
   "outputs": [],
   "source": [
    "del candidates\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F-wxWYRFPd9O"
   },
   "outputs": [],
   "source": [
    "print(\"Train positive rate:\", train.label.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O9JQdMFFPfov"
   },
   "outputs": [],
   "source": [
    "train = train.sort_values(by=[\"week\", \"customer_id\"], ascending=True).reset_index(drop=True)\n",
    "valid = valid.sort_values(by=[\"customer_id\"], ascending=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aj70p1WDPgfQ"
   },
   "outputs": [],
   "source": [
    "train_group = train[[\"customer_id\", \"article_id\", \"week\"]]\n",
    "train_group = train_group.astype(\"int32\")  # convert to int to avoid `0` in groupby count result\n",
    "train_group = (train_group.groupby([\"week\", \"customer_id\"]).size().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lYOh4wBaPhaN"
   },
   "outputs": [],
   "source": [
    "valid_group = valid[[\"customer_id\", \"article_id\"]]\n",
    "valid_group = valid_group.astype(\"int32\")  # convert to int to avoid `0` in groupby count result\n",
    "valid_group = valid_group.groupby([\"customer_id\"]).size().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z1LrjrLKPiYA"
   },
   "outputs": [],
   "source": [
    "train = train[feats+['label']]\n",
    "valid = valid[feats+['label']]\n",
    "\n",
    "# these lines of code select only the features (`feats`) and the target variable (`label`) from the `train` and `valid` DataFrames, discarding all other columns.\n",
    "# this is done to prepare the data for model training, ensuring that only relevant features and the target variable are used in the training and validation processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lm8JNIuqPjMN"
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1FxwqE1cJR4L"
   },
   "outputs": [],
   "source": [
    "classifier = train_classifier_model(train, valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KbFdGrYRdtIt"
   },
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hl3DPTaBGGQV"
   },
   "outputs": [],
   "source": [
    "classifier = lgb.Booster(model_file=model_dir / \"lgb_classifier.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OG9AwSntGGQV"
   },
   "outputs": [],
   "source": [
    "feat_importance = pd.DataFrame(\n",
    "    {\"feature\": feats, \"importance\": classifier.feature_importance()}\n",
    ").sort_values(by=\"importance\", ascending=False)\n",
    "plt.figure(figsize=(8, 22))\n",
    "sns.barplot(y=\"feature\", x=\"importance\", data=feat_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lLWs-Z5CGGQV"
   },
   "source": [
    "### Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SSpgfI0mdtIv"
   },
   "outputs": [],
   "source": [
    "val_candidates = valid.reset_index(drop=True)\n",
    "# creates a new DataFrame named val_candidates by resetting the index of the DataFrame valid and dropping the original index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pEhET3KZJR4N"
   },
   "outputs": [],
   "source": [
    "def predict(classifier, candidates, batch_size = 5_000_000):\n",
    "    probs = np.zeros(candidates.shape[0])\n",
    "    for batch in range(0, candidates.shape[0], batch_size):\n",
    "        outputs = classifier.predict(candidates.loc[batch : batch + batch_size - 1, feats])\n",
    "        probs[batch : batch + batch_size] = outputs\n",
    "    candidates[\"prob\"] = probs\n",
    "    pred_lgb = candidates[['customer_id','article_id','prob']]\n",
    "    pred_lgb = pred_lgb.sort_values(by=[\"customer_id\",\"prob\"], ascending=False).reset_index(drop=True)\n",
    "    pred_lgb.rename(columns={'article_id':'prediction'}, inplace=True)\n",
    "    pred_lgb = pred_lgb.drop_duplicates(['customer_id', 'prediction'], keep='first')\n",
    "    pred_lgb['customer_id'] = pred_lgb['customer_id'].astype(int)\n",
    "    pred_lgb = pred_lgb.groupby(\"customer_id\")[\"prediction\"].progress_apply(list).reset_index()\n",
    "    return pred_lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ifgoemo6JR4N"
   },
   "outputs": [],
   "source": [
    "pred = predict(classifier, val_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G1Qh3k2aGGQW"
   },
   "outputs": [],
   "source": [
    "label = labels[1]\n",
    "label = pd.merge(label, pred, on=\"customer_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XrFnQ2evGGQW"
   },
   "outputs": [],
   "source": [
    "map_at_k(label[\"article_id\"], label[\"prediction\"], k=12)\n",
    "\n",
    "# 0.029813727108367518 ranker\n",
    "# 0.029791925075924913 binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SEpG-KVUJR4O"
   },
   "outputs": [],
   "source": [
    "batch_size = 5_000_000\n",
    "probs = np.zeros(val_candidates.shape[0])\n",
    "for batch in range(0, val_candidates.shape[0], batch_size):\n",
    "    outputs = classifier.predict(val_candidates.loc[batch : batch + batch_size - 1, feats])\n",
    "    probs[batch : batch + batch_size] = outputs\n",
    "val_candidates[\"prob\"] = probs\n",
    "pred_lgb = val_candidates[['customer_id','article_id','prob']]\n",
    "pred_lgb = pred_lgb.sort_values(by=[\"customer_id\",\"prob\"], ascending=False).reset_index(drop=True)\n",
    "pred_lgb.rename(columns={'article_id':'prediction'}, inplace=True)\n",
    "pred_lgb = pred_lgb.drop_duplicates(['customer_id', 'prediction'], keep='first')\n",
    "pred_lgb['customer_id'] = pred_lgb['customer_id'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UILjptYKJR4O"
   },
   "outputs": [],
   "source": [
    "pred_lgb.to_parquet(data_dir/\"processed\"/\"lgb_classifier_valid.pqt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_5_faqlKJR4R"
   },
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qFaPtoF7JR4R"
   },
   "outputs": [],
   "source": [
    "del candidate\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GxTa-32qPMNh"
   },
   "outputs": [],
   "source": [
    "test_pred = []\n",
    "for chunk in range(2):\n",
    "    print(f\"Chunk {chunk}\")\n",
    "    test_candidates = pd.read_parquet(data_dir/\"processed\"/VERSION_NAME/f\"week0_candidate_{chunk}.pqt\")\n",
    "    for feat in cat_features:\n",
    "        test_candidates[feat] = test_candidates[feat].astype(cate_dict[feat])\n",
    "\n",
    "    # Extra Features ===================================\n",
    "\n",
    "    tmp = inter.groupby('article_id').week.mean()\n",
    "    test_candidates['article_time_mean'] = test_candidates['article_id'].map(tmp)\n",
    "\n",
    "    tmp = inter.groupby('customer_id').week.nth(-1)\n",
    "    test_candidates['customer_id_last_time'] = test_candidates['customer_id'].map(tmp)\n",
    "\n",
    "    tmp = inter.groupby('customer_id').week.nth(0)\n",
    "    test_candidates['customer_id_first_time'] = test_candidates['customer_id'].map(tmp)\n",
    "\n",
    "    tmp = inter.groupby('customer_id').week.mean()\n",
    "    test_candidates['customer_id_time_mean'] = test_candidates['customer_id'].map(tmp)\n",
    "\n",
    "    test_candidates['customer_id_gap'] = test_candidates['customer_id_first_time'] - test_candidates['customer_id_last_time']\n",
    "\n",
    "    gc.collect()\n",
    "    # ==================================================\n",
    "\n",
    "    batch_size = 5_000_000\n",
    "    probs = np.zeros(test_candidates.shape[0])\n",
    "    for batch in tqdm(range(0, test_candidates.shape[0], batch_size)):\n",
    "        outputs = classifier.predict(test_candidates.loc[batch : batch + batch_size - 1, feats])\n",
    "        probs[batch : batch + batch_size] = outputs\n",
    "    test_candidates[\"prob\"] = probs\n",
    "    pred_lgb = test_candidates[['customer_id','article_id','prob']]\n",
    "    pred_lgb = pred_lgb.sort_values(by=[\"customer_id\",\"prob\"], ascending=False).reset_index(drop=True)\n",
    "    pred_lgb.rename(columns={'article_id':'prediction'}, inplace=True)\n",
    "    pred_lgb = pred_lgb.drop_duplicates(['customer_id', 'prediction'], keep='first')\n",
    "    pred_lgb['customer_id'] = pred_lgb['customer_id'].astype(int)\n",
    "    test_pred.append(pred_lgb)\n",
    "    del test_candidates\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S1D0jEFqPzng"
   },
   "outputs": [],
   "source": [
    "pred_lgb = pd.concat(test_pred, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZpdiPY58JR4R"
   },
   "outputs": [],
   "source": [
    "pred_lgb.to_parquet(data_dir/\"processed\"/\"lgb_classifier_test.pqt\")\n",
    "\n",
    "# Saves the predictions for the test data into a Parquet file named \"lgb_classifier_test.pqt\" in the \"processed\" directory of the specified `data_dir`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kuFZgf0WZHNj"
   },
   "outputs": [],
   "source": [
    "pred_lgb.head()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "interpreter": {
   "hash": "eee4fdb9ca52ce5d5f0a2a1c2a1d0a4896d6b735579ddf3d9c0ee93e21b97ca8"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
